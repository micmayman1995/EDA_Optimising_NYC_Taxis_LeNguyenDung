{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQl5n2IOuafL"
   },
   "source": [
    "# **New York City Yellow Taxi Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGQVIB4mEFrZ"
   },
   "source": [
    "## Objective\n",
    "In this case study you will be learning exploratory data analysis (EDA) with the help of a dataset on yellow taxi rides in New York City. This will enable you to understand why EDA is an important step in the process of data science and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJVMenVllLUL"
   },
   "source": [
    "## **Problem Statement**\n",
    "As an analyst at an upcoming taxi operation in NYC, you are tasked to use the 2023 taxi trip data to uncover insights that could help optimise taxi operations. The goal is to analyse patterns in the data that can inform strategic decisions to improve service efficiency, maximise revenue, and enhance passenger experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OVfUMlHFkZD"
   },
   "source": [
    "## Tasks\n",
    "You need to perform the following steps for successfully completing this assignment:\n",
    "1. Data Loading\n",
    "2. Data Cleaning\n",
    "3. Exploratory Analysis: Bivariate and Multivariate\n",
    "4. Creating Visualisations to Support the Analysis\n",
    "5. Deriving Insights and Stating Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTxV-3GJUhWm"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofebI8ITG-Li"
   },
   "source": [
    "**NOTE:** The marks given along with headings and sub-headings are cumulative marks for those particular headings/sub-headings.<br>\n",
    "\n",
    "The actual marks for each task are specified within the tasks themselves.\n",
    "\n",
    "For example, marks given with heading *2* or sub-heading *2.1* are the cumulative marks, for your reference only. <br>\n",
    "\n",
    "The marks you will receive for completing tasks are given with the tasks.\n",
    "\n",
    "Suppose the marks for two tasks are: 3 marks for 2.1.1 and 2 marks for 3.2.2, or\n",
    "* 2.1.1 [3 marks]\n",
    "* 3.2.2 [2 marks]\n",
    "\n",
    "then, you will earn 3 marks for completing task 2.1.1 and 2 marks for completing task 3.2.2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdQjht7dUiHt"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eaCZjHIvfuI"
   },
   "source": [
    "## Data Understanding\n",
    "The yellow taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts.\n",
    "\n",
    "The data is stored in Parquet format (*.parquet*). The dataset is from 2009 to 2024. However, for this assignment, we will only be using the data from 2023.\n",
    "\n",
    "The data for each month is present in a different parquet file. You will get twelve files for each of the months in 2023.\n",
    "\n",
    "The data was collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers like vendors and taxi hailing apps. <br>\n",
    "\n",
    "You can find the link to the TLC trip records page here: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LI6qC5IDxZU1"
   },
   "source": [
    "###  Data Description\n",
    "You can find the data description here: [Data Dictionary](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FafCzrDuxHg2"
   },
   "source": [
    "**Trip Records**\n",
    "\n",
    "\n",
    "\n",
    "|Field Name       |description |\n",
    "|:----------------|:-----------|\n",
    "| VendorID | A code indicating the TPEP provider that provided the record. <br> 1= Creative Mobile Technologies, LLC; <br> 2= VeriFone Inc. |\n",
    "| tpep_pickup_datetime | The date and time when the meter was engaged.  |\n",
    "| tpep_dropoff_datetime | The date and time when the meter was disengaged.   |\n",
    "| Passenger_count | The number of passengers in the vehicle. <br> This is a driver-entered value. |\n",
    "| Trip_distance | The elapsed trip distance in miles reported by the taximeter. |\n",
    "| PULocationID | TLC Taxi Zone in which the taximeter was engaged |\n",
    "| DOLocationID | TLC Taxi Zone in which the taximeter was disengaged |\n",
    "|RateCodeID |The final rate code in effect at the end of the trip.<br> 1 = Standard rate <br> 2 = JFK <br> 3 = Newark <br>4 = Nassau or Westchester <br>5 = Negotiated fare <br>6 = Group ride |\n",
    "|Store_and_fwd_flag |This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,” because the vehicle did not have a connection to the server.  <br>Y= store and forward trip <br>N= not a store and forward trip |\n",
    "|Payment_type| A numeric code signifying how the passenger paid for the trip. <br> 1 = Credit card <br>2 = Cash <br>3 = No charge <br>4 = Dispute <br>5 = Unknown <br>6 = Voided trip |\n",
    "|Fare_amount| The time-and-distance fare calculated by the meter. <br>Extra Miscellaneous extras and surcharges.  Currently, this only includes the 0.50 and 1 USD rush hour and overnight charges. |\n",
    "|MTA_tax |0.50 USD MTA tax that is automatically triggered based on the metered rate in use. |\n",
    "|Improvement_surcharge | 0.30 USD improvement surcharge assessed trips at the flag drop. The improvement surcharge began being levied in 2015. |\n",
    "|Tip_amount |Tip amount – This field is automatically populated for credit card tips. Cash tips are not included. |\n",
    "| Tolls_amount | Total amount of all tolls paid in trip.  |\n",
    "| total_amount | The total amount charged to passengers. Does not include cash tips. |\n",
    "|Congestion_Surcharge |Total amount collected in trip for NYS congestion surcharge. |\n",
    "| Airport_fee | 1.25 USD for pick up only at LaGuardia and John F. Kennedy Airports|\n",
    "\n",
    "Although the amounts of extra charges and taxes applied are specified in the data dictionary, you will see that some cases have different values of these charges in the actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mL-FWYFnVEdE"
   },
   "source": [
    "**Taxi Zones**\n",
    "\n",
    "Each of the trip records contains a field corresponding to the location of the pickup or drop-off of the trip, populated by numbers ranging from 1-263.\n",
    "\n",
    "These numbers correspond to taxi zones, which may be downloaded as a table or map/shapefile and matched to the trip records using a join.\n",
    "\n",
    "This is covered in more detail in later sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z66W3s51U0gF"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kw-WRzBfyS7j"
   },
   "source": [
    "## **1** Data Preparation\n",
    "\n",
    "<font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nM2X-s6lycvQ"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "juReqsAzEdW3"
   },
   "outputs": [],
   "source": [
    "# Import warnings\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "3XZjOlJiy1dr"
   },
   "outputs": [],
   "source": [
    "# Import the libraries you will be using for analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "NsH5LNrSgW9q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "pandas version: 2.2.2\n",
      "matplotlib version: 3.10.1\n",
      "seaborn version: 0.13.2\n"
     ]
    }
   ],
   "source": [
    "# Recommended versions\n",
    "# numpy version: 1.26.4\n",
    "# pandas version: 2.2.2\n",
    "# matplotlib version: 3.10.0\n",
    "# seaborn version: 0.13.2\n",
    "\n",
    "# Check versions\n",
    "print(\"numpy version:\", np.__version__)\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"matplotlib version:\", plt.matplotlib.__version__)\n",
    "print(\"seaborn version:\", sns.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgHgbPIepaYl"
   },
   "source": [
    "### **1.1** Load the dataset\n",
    "<font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrGluF_gpeHs"
   },
   "source": [
    "You will see twelve files, one for each month.\n",
    "\n",
    "To read parquet files with Pandas, you have to follow a similar syntax as that for CSV files.\n",
    "\n",
    "`df = pd.read_parquet('file.parquet')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "kIpIsuSSzCp9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3066766 entries, 0 to 3066765\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Dtype         \n",
      "---  ------                 -----         \n",
      " 0   VendorID               int64         \n",
      " 1   tpep_pickup_datetime   datetime64[us]\n",
      " 2   tpep_dropoff_datetime  datetime64[us]\n",
      " 3   passenger_count        float64       \n",
      " 4   trip_distance          float64       \n",
      " 5   RatecodeID             float64       \n",
      " 6   store_and_fwd_flag     object        \n",
      " 7   PULocationID           int64         \n",
      " 8   DOLocationID           int64         \n",
      " 9   payment_type           int64         \n",
      " 10  fare_amount            float64       \n",
      " 11  extra                  float64       \n",
      " 12  mta_tax                float64       \n",
      " 13  tip_amount             float64       \n",
      " 14  tolls_amount           float64       \n",
      " 15  improvement_surcharge  float64       \n",
      " 16  total_amount           float64       \n",
      " 17  congestion_surcharge   float64       \n",
      " 18  airport_fee            float64       \n",
      "dtypes: datetime64[us](2), float64(12), int64(4), object(1)\n",
      "memory usage: 444.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Try loading one file\n",
    "data_path = ('C:\\\\Users\\\\Admin\\OneDrive\\Mic\\Study\\Master\\\\NYC Taxi\\Data\\\\')\n",
    "df = pd.read_parquet(data_path + 'yellow_tripdata_2023-01.parquet')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eh-keWIyqcyr"
   },
   "source": [
    "How many rows are there? Do you think handling such a large number of rows is computationally feasible when we have to combine the data for all twelve months into one?\n",
    "\n",
    "To handle this, we need to sample a fraction of data from each of the files. How to go about that? Think of a way to select only some portion of the data from each month's file that accurately represents the trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHx7lh_3QGmj"
   },
   "source": [
    "#### Sampling the Data\n",
    "> One way is to take a small percentage of entries for pickup in every hour of a date. So, for all the days in a month, we can iterate through the hours and select 5% values randomly from those. Use `tpep_pickup_datetime` for this. Separate date and hour from the datetime values and then for each date, select some fraction of trips for each of the 24 hours.\n",
    "\n",
    "To sample data, you can use the `sample()` method. Follow this syntax:\n",
    "\n",
    "```Python\n",
    "# sampled_data is an empty DF to keep appending sampled data of each hour\n",
    "# hour_data is the DF of entries for an hour 'X' on a date 'Y'\n",
    "\n",
    "sample = hour_data.sample(frac = 0.05, random_state = 42)\n",
    "# sample 0.05 of the hour_data\n",
    "# random_state is just a seed for sampling, you can define it yourself\n",
    "\n",
    "sampled_data = pd.concat([sampled_data, sample]) # adding data for this hour to the DF\n",
    "```\n",
    "\n",
    "This *sampled_data* will contain 5% values selected at random from each hour.\n",
    "\n",
    "Note that the code given above is only the part that will be used for sampling and not the complete code required for sampling and combining the data files.\n",
    "\n",
    "Keep in mind that you sample by date AND hour, not just hour. (Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zog80nsqvKp"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fp24-wpQrlC5"
   },
   "source": [
    "**1.1.1** <font color = red>[5 marks]</font> <br>\n",
    "Figure out how to sample and combine the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRGOnciGOvq0"
   },
   "source": [
    "**Note:** It is not mandatory to use the method specified above. While sampling, you only need to make sure that your sampled data represents the overall data of all the months accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "3cpuzPFvP2iC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows = 3376567\n",
      "Sampled number of rows = 168836\n",
      "Biggest proportion difference = 3.079138580389367e-06\n",
      "Average proportion difference = 1.4943648262555587e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2544\\3659038602.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_df = df.groupby(base_col, group_keys=False).apply(lambda x: x.sample(frac=frac), include_groups=include_groups)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>Airport_fee</th>\n",
       "      <th>pickup_date_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3682</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-11-30 23:56:07</td>\n",
       "      <td>2023-12-01 00:08:17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.060000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>N</td>\n",
       "      <td>231</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-11-30 23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2336</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-11-30 23:53:20</td>\n",
       "      <td>2023-12-01 00:19:37</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.140000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>N</td>\n",
       "      <td>148</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>45.600000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-11-30 23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-12-01 00:45:50</td>\n",
       "      <td>2023-12-01 00:59:36</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>N</td>\n",
       "      <td>141</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-12-01 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-12-01 00:22:27</td>\n",
       "      <td>2023-12-01 00:33:33</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>N</td>\n",
       "      <td>87</td>\n",
       "      <td>249</td>\n",
       "      <td>1</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-12-01 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2256</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-12-01 00:38:53</td>\n",
       "      <td>2023-12-01 00:43:16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>N</td>\n",
       "      <td>234</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.440000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.640000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-12-01 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "3682         2  2023-11-30 23:56:07   2023-12-01 00:08:17         1.000000   \n",
       "2336         2  2023-11-30 23:53:20   2023-12-01 00:19:37         1.000000   \n",
       "2031         2  2023-12-01 00:45:50   2023-12-01 00:59:36         2.000000   \n",
       "1620         1  2023-12-01 00:22:27   2023-12-01 00:33:33         2.000000   \n",
       "2256         2  2023-12-01 00:38:53   2023-12-01 00:43:16         1.000000   \n",
       "\n",
       "      trip_distance  RatecodeID store_and_fwd_flag  PULocationID  \\\n",
       "3682       2.060000    1.000000                  N           231   \n",
       "2336       8.140000    1.000000                  N           148   \n",
       "2031       3.250000    1.000000                  N           141   \n",
       "1620       2.200000    1.000000                  N            87   \n",
       "2256       1.030000    1.000000                  N           234   \n",
       "\n",
       "      DOLocationID  payment_type  fare_amount    extra  mta_tax  tip_amount  \\\n",
       "3682             4             1    13.500000 1.000000 0.500000    3.700000   \n",
       "2336           238             1    36.600000 1.000000 0.500000    4.000000   \n",
       "2031           226             1    17.000000 1.000000 0.500000    2.000000   \n",
       "1620           249             1    13.500000 3.500000 0.500000    3.700000   \n",
       "2256            79             1     7.200000 1.000000 0.500000    2.440000   \n",
       "\n",
       "      tolls_amount  improvement_surcharge  total_amount  congestion_surcharge  \\\n",
       "3682      0.000000               1.000000     22.200000              2.500000   \n",
       "2336      0.000000               1.000000     45.600000              2.500000   \n",
       "2031      0.000000               1.000000     24.000000              2.500000   \n",
       "1620      0.000000               1.000000     22.200000              2.500000   \n",
       "2256      0.000000               1.000000     14.640000              2.500000   \n",
       "\n",
       "      Airport_fee pickup_date_hour  \n",
       "3682     0.000000    2023-11-30 23  \n",
       "2336     0.000000    2023-11-30 23  \n",
       "2031     0.000000     2023-12-01 0  \n",
       "1620     0.000000     2023-12-01 0  \n",
       "2256     0.000000     2023-12-01 0  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample the data\n",
    "# It is recommmended to not load all the files at once to avoid memory overload\n",
    "def get_sample(df, base_col, frac=0.05, include_groups=True):\n",
    "    sampled_df = df.groupby(base_col, group_keys=False).apply(lambda x: x.sample(frac=frac), include_groups=include_groups)\n",
    "    return sampled_df\n",
    "\n",
    "\n",
    "sample = df.copy()\n",
    "print(f'Original number of rows = {sample.shape[0]}')\n",
    "\n",
    "# Sample 5% of the original data using pickup date and hour as bases to keep the distribution\n",
    "sample['pickup_date_hour'] = sample['tpep_pickup_datetime'].dt.date.astype(str) + ' ' + sample['tpep_pickup_datetime'].dt.hour.astype(str)\n",
    "\n",
    "before_dist = sample.groupby(['pickup_date_hour']).size().reset_index(name='count_before')\n",
    "before_dist['proportion_before'] = before_dist['count_before'] / before_dist['count_before'].sum() # For comparison purposes\n",
    "\n",
    "sample = get_sample(sample, 'pickup_date_hour', 0.05, include_groups=True)\n",
    "\n",
    "after_dist = sample.groupby(['pickup_date_hour']).size().reset_index(name='count_after')\n",
    "after_dist['proportion_after'] = after_dist['count_after'] / after_dist['count_after'].sum() # For comparison purposes\n",
    "\n",
    "comparison_dist = before_dist.merge(after_dist, how='outer', on='pickup_date_hour') # Create another dataframe to compare the before vs after distribution\n",
    "comparison_dist['count_after'] = comparison_dist['count_after'].fillna(0).astype(int)\n",
    "comparison_dist['proportion_after'] = comparison_dist['proportion_after'].fillna(0)\n",
    "comparison_dist['abs_proportion_diff'] = abs(comparison_dist['proportion_before'] - comparison_dist['proportion_after'])\n",
    "print(f'Sampled number of rows = {sample.shape[0]}')\n",
    "\n",
    "print(f'Biggest proportion difference = {comparison_dist.abs_proportion_diff.max()}')\n",
    "print(f'Average proportion difference = {comparison_dist.abs_proportion_diff.mean()}')\n",
    "\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3x5bCWmarTO"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1EXP0PHzPs0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now reading yellow_tripdata_2023-01.parquet\n",
      "Sampled 21456 out of 3066766 rows\n",
      "Now reading yellow_tripdata_2023-02.parquet\n",
      "Sampled 20405 out of 2913955 rows\n",
      "Now reading yellow_tripdata_2023-03.parquet\n",
      "Sampled 23834 out of 3403766 rows\n",
      "Now reading yellow_tripdata_2023-04.parquet\n",
      "Sampled 23003 out of 3288250 rows\n",
      "Now reading yellow_tripdata_2023-05.parquet\n",
      "Sampled 24590 out of 3513649 rows\n",
      "Now reading yellow_tripdata_2023-06.parquet\n",
      "Sampled 23136 out of 3307234 rows\n",
      "Now reading yellow_tripdata_2023-07.parquet\n",
      "Sampled 20366 out of 2907108 rows\n",
      "Now reading yellow_tripdata_2023-08.parquet\n",
      "Sampled 19759 out of 2824209 rows\n",
      "Now reading yellow_tripdata_2023-09.parquet\n",
      "Sampled 19936 out of 2846722 rows\n",
      "Now reading yellow_tripdata_2023-10.parquet\n",
      "Sampled 24643 out of 3522285 rows\n",
      "Now reading yellow_tripdata_2023-11.parquet\n",
      "Sampled 23390 out of 3339715 rows\n",
      "Now reading yellow_tripdata_2023-12.parquet\n",
      "Sampled 23632 out of 3376567 rows\n",
      "Concatination Completed\n",
      "The concated file has 268150 rows\n"
     ]
    }
   ],
   "source": [
    "# Take a small percentage of entries from each hour of every date.\n",
    "# Iterating through the monthly data:\n",
    "#   read a month file -> day -> hour: append sampled data -> move to next hour -> move to next day after 24 hours -> move to next month file\n",
    "# Create a single dataframe for the year combining all the monthly data\n",
    "\n",
    "# Select the folder having data files\n",
    "import os\n",
    "\n",
    "# Select the folder having data files\n",
    "os.chdir('C:\\\\Users\\\\Admin\\OneDrive\\Mic\\Study\\Master\\\\NYC Taxi\\Data\\\\')\n",
    "\n",
    "# Create a list of all the twelve files to read\n",
    "file_list = [x for x in os.listdir() if 'parquet' in x]\n",
    "\n",
    "# initialise an empty dataframe\n",
    "concat_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "# iterate through the list of files and sample one by one:\n",
    "for file_name in file_list:\n",
    "    try:\n",
    "        # Reading the current file\n",
    "        print(f'Now reading {file_name}')\n",
    "        df = pd.read_parquet(file_name)\n",
    "        origin_rows = df.shape[0]\n",
    "\n",
    "        # Create the column to use as the distribution base\n",
    "        df['pickup_date_hour'] = df['tpep_pickup_datetime'].dt.date.astype(str) + ' ' + df['tpep_pickup_datetime'].dt.hour.astype(str) \n",
    "        # Sampling 0.07% of the data to keep the final total entries at ~270,000 rows\n",
    "        sampled_data = get_sample(df, 'pickup_date_hour', 0.007, include_groups=False)\n",
    "        \n",
    "        sampled_rows = sampled_data.shape[0]\n",
    "        print(f'Sampled {sampled_rows} out of {origin_rows} rows')\n",
    "        # Concatenate the sampled data of all the dates to a single dataframe\n",
    "        concat_df = pd.concat([concat_df, sampled_data])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_name}: {e}\")\n",
    "\n",
    "print('Concatination Completed')\n",
    "print(f'The concated file has {concat_df.shape[0]} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sej6pZkzw2AK"
   },
   "source": [
    "After combining the data files into one DataFrame, convert the new DataFrame to a CSV or parquet file and store it to use directly.\n",
    "\n",
    "Ideally, you can try keeping the total entries to around 250,000 to 300,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "okaVAsdPxJow"
   },
   "outputs": [],
   "source": [
    "# Store the df in csv/parquet\n",
    "concat_df.to_parquet(data_path + 'concat_file\\\\yellow_tripdata_concat.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaOS3H9izZ0N"
   },
   "source": [
    "## **2** Data Cleaning\n",
    "<font color = red>[30 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5y3PKWqhxRA9"
   },
   "source": [
    "Now we can load the new data directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "FOuOL0SDxQHd"
   },
   "outputs": [],
   "source": [
    "# Load the new data file\n",
    "df_raw = pd.read_parquet('C:\\\\Users\\\\Admin\\OneDrive\\Mic\\Study\\Master\\\\NYC Taxi\\Data\\\\concat_file\\\\yellow_tripdata_concat.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "fbzmFKyn1780"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>Airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4585</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01 00:23:23</td>\n",
       "      <td>2023-01-01 00:45:35</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>N</td>\n",
       "      <td>239</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.550000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.950000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01 00:21:57</td>\n",
       "      <td>2023-01-01 00:32:21</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.470000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>N</td>\n",
       "      <td>141</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01 00:55:12</td>\n",
       "      <td>2023-01-02 00:52:12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.430000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>N</td>\n",
       "      <td>170</td>\n",
       "      <td>239</td>\n",
       "      <td>2</td>\n",
       "      <td>21.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.200000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995299</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01 00:54:00</td>\n",
       "      <td>2023-01-01 01:27:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>143</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>36.440000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.090000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.530000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01 00:12:09</td>\n",
       "      <td>2023-01-01 00:36:30</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>N</td>\n",
       "      <td>239</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>36.450000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "4585            1  2023-01-01 00:23:23   2023-01-01 00:45:35         1.000000   \n",
       "1197            2  2023-01-01 00:21:57   2023-01-01 00:32:21         1.000000   \n",
       "1244            2  2023-01-01 00:55:12   2023-01-02 00:52:12         1.000000   \n",
       "2995299         2  2023-01-01 00:54:00   2023-01-01 01:27:00              NaN   \n",
       "2338            1  2023-01-01 00:12:09   2023-01-01 00:36:30         1.000000   \n",
       "\n",
       "         trip_distance  RatecodeID store_and_fwd_flag  PULocationID  \\\n",
       "4585          5.400000    1.000000                  N           239   \n",
       "1197          1.470000    1.000000                  N           141   \n",
       "1244          3.430000    1.000000                  N           170   \n",
       "2995299       0.240000         NaN               None           143   \n",
       "2338          4.400000    1.000000                  N           239   \n",
       "\n",
       "         DOLocationID  payment_type  fare_amount    extra  mta_tax  \\\n",
       "4585              107             1    25.400000 3.500000 0.500000   \n",
       "1197              141             2    11.400000 1.000000 0.500000   \n",
       "1244              239             2    21.200000 1.000000 0.500000   \n",
       "2995299           148             0    36.440000 0.000000 0.500000   \n",
       "2338               90             1    25.400000 3.500000 0.500000   \n",
       "\n",
       "         tip_amount  tolls_amount  improvement_surcharge  total_amount  \\\n",
       "4585       7.550000      0.000000               1.000000     37.950000   \n",
       "1197       0.000000      0.000000               1.000000     16.400000   \n",
       "1244       0.000000      0.000000               1.000000     26.200000   \n",
       "2995299    8.090000      0.000000               1.000000     48.530000   \n",
       "2338       6.050000      0.000000               1.000000     36.450000   \n",
       "\n",
       "         congestion_surcharge  airport_fee  Airport_fee  \n",
       "4585                 2.500000     0.000000          NaN  \n",
       "1197                 2.500000     0.000000          NaN  \n",
       "1244                 2.500000     0.000000          NaN  \n",
       "2995299                   NaN          NaN          NaN  \n",
       "2338                 2.500000     0.000000          NaN  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "FH83U4A49ErC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 268150 entries, 4585 to 3138963\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Non-Null Count   Dtype         \n",
      "---  ------                 --------------   -----         \n",
      " 0   VendorID               268150 non-null  int64         \n",
      " 1   tpep_pickup_datetime   268150 non-null  datetime64[us]\n",
      " 2   tpep_dropoff_datetime  268150 non-null  datetime64[us]\n",
      " 3   passenger_count        258945 non-null  float64       \n",
      " 4   trip_distance          268150 non-null  float64       \n",
      " 5   RatecodeID             258945 non-null  float64       \n",
      " 6   store_and_fwd_flag     258945 non-null  object        \n",
      " 7   PULocationID           268150 non-null  int64         \n",
      " 8   DOLocationID           268150 non-null  int64         \n",
      " 9   payment_type           268150 non-null  int64         \n",
      " 10  fare_amount            268150 non-null  float64       \n",
      " 11  extra                  268150 non-null  float64       \n",
      " 12  mta_tax                268150 non-null  float64       \n",
      " 13  tip_amount             268150 non-null  float64       \n",
      " 14  tolls_amount           268150 non-null  float64       \n",
      " 15  improvement_surcharge  268150 non-null  float64       \n",
      " 16  total_amount           268150 non-null  float64       \n",
      " 17  congestion_surcharge   258945 non-null  float64       \n",
      " 18  airport_fee            20964 non-null   float64       \n",
      " 19  Airport_fee            237981 non-null  float64       \n",
      "dtypes: datetime64[us](2), float64(13), int64(4), object(1)\n",
      "memory usage: 43.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZvPSwJx0S3K"
   },
   "source": [
    "#### **2.1** Fixing Columns\n",
    "<font color = red>[10 marks]</font> <br>\n",
    "\n",
    "Fix/drop any columns as you seem necessary in the below sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "358takCd2FiM"
   },
   "source": [
    "**2.1.1** <font color = red>[2 marks]</font> <br>\n",
    "\n",
    "Fix the index and drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "KHHFyZxa2PEM"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>Airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 00:23:23</td>\n",
       "      <td>2023-01-01 00:45:35</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>239</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>7.550000</td>\n",
       "      <td>37.950000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 00:21:57</td>\n",
       "      <td>2023-01-01 00:32:21</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.470000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>141</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 00:55:12</td>\n",
       "      <td>2023-01-02 00:52:12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.430000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>170</td>\n",
       "      <td>239</td>\n",
       "      <td>2</td>\n",
       "      <td>21.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 00:54:00</td>\n",
       "      <td>2023-01-01 01:27:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>143</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>36.440000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.090000</td>\n",
       "      <td>48.530000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 00:12:09</td>\n",
       "      <td>2023-01-01 00:36:30</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>239</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>6.050000</td>\n",
       "      <td>36.450000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268145</th>\n",
       "      <td>2023-12-31 09:49:26</td>\n",
       "      <td>2023-12-31 10:25:41</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.900000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>259</td>\n",
       "      <td>124</td>\n",
       "      <td>1</td>\n",
       "      <td>75.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.880000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268146</th>\n",
       "      <td>2023-12-31 09:59:03</td>\n",
       "      <td>2023-12-31 10:10:58</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>170</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268147</th>\n",
       "      <td>2023-12-31 09:48:55</td>\n",
       "      <td>2023-12-31 09:56:42</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>163</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268148</th>\n",
       "      <td>2023-12-31 09:10:03</td>\n",
       "      <td>2023-12-31 09:52:12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>50</td>\n",
       "      <td>186</td>\n",
       "      <td>1</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268149</th>\n",
       "      <td>2023-12-31 09:59:15</td>\n",
       "      <td>2023-12-31 10:12:01</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.180000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>162</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>26.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.020000</td>\n",
       "      <td>36.120000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>268150 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0       2023-01-01 00:23:23   2023-01-01 00:45:35         1.000000   \n",
       "1       2023-01-01 00:21:57   2023-01-01 00:32:21         1.000000   \n",
       "2       2023-01-01 00:55:12   2023-01-02 00:52:12         1.000000   \n",
       "3       2023-01-01 00:54:00   2023-01-01 01:27:00              NaN   \n",
       "4       2023-01-01 00:12:09   2023-01-01 00:36:30         1.000000   \n",
       "...                     ...                   ...              ...   \n",
       "268145  2023-12-31 09:49:26   2023-12-31 10:25:41         1.000000   \n",
       "268146  2023-12-31 09:59:03   2023-12-31 10:10:58         4.000000   \n",
       "268147  2023-12-31 09:48:55   2023-12-31 09:56:42         1.000000   \n",
       "268148  2023-12-31 09:10:03   2023-12-31 09:52:12         1.000000   \n",
       "268149  2023-12-31 09:59:15   2023-12-31 10:12:01         2.000000   \n",
       "\n",
       "        trip_distance  RatecodeID  PULocationID  DOLocationID  payment_type  \\\n",
       "0            5.400000    1.000000           239           107             1   \n",
       "1            1.470000    1.000000           141           141             2   \n",
       "2            3.430000    1.000000           170           239             2   \n",
       "3            0.240000         NaN           143           148             0   \n",
       "4            4.400000    1.000000           239            90             1   \n",
       "...               ...         ...           ...           ...           ...   \n",
       "268145      20.900000   99.000000           259           124             1   \n",
       "268146       3.100000    1.000000           170           239             1   \n",
       "268147       1.600000    1.000000           163           236             1   \n",
       "268148       1.100000   99.000000            50           186             1   \n",
       "268149       6.180000    1.000000           162           261             1   \n",
       "\n",
       "        fare_amount    extra  tip_amount  total_amount  airport_fee  \\\n",
       "0         25.400000 3.500000    7.550000     37.950000     0.000000   \n",
       "1         11.400000 1.000000    0.000000     16.400000     0.000000   \n",
       "2         21.200000 1.000000    0.000000     26.200000     0.000000   \n",
       "3         36.440000 0.000000    8.090000     48.530000          NaN   \n",
       "4         25.400000 3.500000    6.050000     36.450000     0.000000   \n",
       "...             ...      ...         ...           ...          ...   \n",
       "268145    75.500000 0.000000    0.000000     90.880000          NaN   \n",
       "268146    15.600000 2.500000    4.900000     24.500000          NaN   \n",
       "268147    10.000000 2.500000    3.500000     17.500000          NaN   \n",
       "268148    17.500000 0.000000    0.000000     19.000000          NaN   \n",
       "268149    26.100000 0.000000    6.020000     36.120000          NaN   \n",
       "\n",
       "        Airport_fee  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  \n",
       "...             ...  \n",
       "268145     0.000000  \n",
       "268146     0.000000  \n",
       "268147     0.000000  \n",
       "268148     0.000000  \n",
       "268149     0.000000  \n",
       "\n",
       "[268150 rows x 14 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix the index and drop any columns that are not needed\n",
    "df = df_raw.copy()\n",
    "df.reset_index(inplace=True, drop=True) # Fix the index\n",
    "df.drop(columns='VendorID', inplace=True) # We don't care about the vendor which the data came from\n",
    "df.drop(columns='store_and_fwd_flag', inplace=True) # Metadata about whether the rows were stored and forwarded, not related to our analysis\n",
    "df.drop(columns='mta_tax', inplace=True) # Are static ~98% of the time\n",
    "df.drop(columns='tolls_amount', inplace=True) # Are static ~92% of the time, only useful for analyzing bridge/tunnel usage\n",
    "df.drop(columns='improvement_surcharge', inplace=True) # Are static ~99% of the time\n",
    "df.drop(columns='congestion_surcharge', inplace=True) # Are static ~88% of the time, only useful for analyzing congestion pricing\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.85493194107775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tolls_amount\n",
       "0.000000      246309\n",
       "6.550000       11859\n",
       "6.940000        8052\n",
       "12.750000        266\n",
       "14.750000        241\n",
       "               ...  \n",
       "22.150000          1\n",
       "39.500000          1\n",
       "-36.050000         1\n",
       "6.220000           1\n",
       "15.760000          1\n",
       "Name: count, Length: 299, dtype: int64"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_value_count = df_raw['tolls_amount'].value_counts().iloc[0]\n",
    "total_count = len(df_raw['tolls_amount'])\n",
    "\n",
    "percentage = most_frequent_value_count / total_count * 100\n",
    "print(percentage)\n",
    "df_raw['tolls_amount'].value_counts(dropna=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIFmxvIT2wsn"
   },
   "source": [
    "**2.1.2** <font color = red>[3 marks]</font> <br>\n",
    "There are two airport fee columns. This is possibly an error in naming columns. Let's see whether these can be combined into a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "id": "rmtgnb1x6TrV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column airport_fee has 247186 missing values\n",
      "Column airport_fee has 30169 missing values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>Airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.250000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.250000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21456</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21460</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21764</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68678</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69091</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       airport_fee  Airport_fee\n",
       "0         0.000000          NaN\n",
       "3              NaN          NaN\n",
       "24       -1.250000          NaN\n",
       "25        1.250000          NaN\n",
       "21456          NaN     0.000000\n",
       "21460          NaN     1.250000\n",
       "21764          NaN    -1.250000\n",
       "68678          NaN     1.750000\n",
       "69091          NaN    -1.750000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column airport_fee has 9205 missing values after combining\n"
     ]
    }
   ],
   "source": [
    "# Combine the two airport fee columns\n",
    "# Check the number of missing values from both columns\n",
    "print(f'Column airport_fee has {df.airport_fee.isna().sum()} missing values')\n",
    "print(f'Column airport_fee has {df.Airport_fee.isna().sum()} missing values')\n",
    "# Confirming that the two columns have no conflicting values\n",
    "display(df.loc[df['airport_fee'] != df['Airport_fee']][['airport_fee', 'Airport_fee']].drop_duplicates())\n",
    "# We will keep the airport_fee column as its naming convention is more in line with other columns, and fill the missing values using Airport_fee\n",
    "df['airport_fee'] = df['airport_fee'].fillna(df['Airport_fee'])\n",
    "df.drop(columns='Airport_fee', inplace=True)\n",
    "# Check the number of missing values from the combined column\n",
    "print(f'Column airport_fee has {df.airport_fee.isna().sum()} missing values after combining')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1g4AHG7mOYgP"
   },
   "source": [
    "**2.1.3** <font color = red>[5 marks]</font> <br>\n",
    "Fix columns with negative (monetary) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "id": "nBsMT2MII1Hv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fare_amount column contains 2641 (0.98%) negative values out of 268150\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2023-01-01 00:24:36</td>\n",
       "      <td>2023-01-01 00:47:45</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.790000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>132</td>\n",
       "      <td>265</td>\n",
       "      <td>4</td>\n",
       "      <td>-57.600000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-61.350000</td>\n",
       "      <td>-1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2023-01-01 10:41:48</td>\n",
       "      <td>2023-01-01 10:46:23</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>79</td>\n",
       "      <td>211</td>\n",
       "      <td>2</td>\n",
       "      <td>-7.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-11.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>2023-01-01 13:38:56</td>\n",
       "      <td>2023-01-01 13:45:24</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>249</td>\n",
       "      <td>234</td>\n",
       "      <td>2</td>\n",
       "      <td>-9.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-13.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>2023-01-01 16:22:21</td>\n",
       "      <td>2023-01-01 16:46:04</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.220000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>145</td>\n",
       "      <td>163</td>\n",
       "      <td>4</td>\n",
       "      <td>-70.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-81.800000</td>\n",
       "      <td>-1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>2023-01-01 03:56:46</td>\n",
       "      <td>2023-01-01 04:35:16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.170000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>144</td>\n",
       "      <td>181</td>\n",
       "      <td>4</td>\n",
       "      <td>-55.500000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-60.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "24   2023-01-01 00:24:36   2023-01-01 00:47:45         3.000000   \n",
       "87   2023-01-01 10:41:48   2023-01-01 10:46:23         1.000000   \n",
       "162  2023-01-01 13:38:56   2023-01-01 13:45:24         2.000000   \n",
       "252  2023-01-01 16:22:21   2023-01-01 16:46:04         3.000000   \n",
       "474  2023-01-01 03:56:46   2023-01-01 04:35:16         1.000000   \n",
       "\n",
       "     trip_distance  RatecodeID  PULocationID  DOLocationID  payment_type  \\\n",
       "24        9.790000    4.000000           132           265             4   \n",
       "87        0.840000    1.000000            79           211             2   \n",
       "162       1.380000    1.000000           249           234             2   \n",
       "252       4.220000    2.000000           145           163             4   \n",
       "474      12.170000    1.000000           144           181             4   \n",
       "\n",
       "     fare_amount     extra  tip_amount  total_amount  airport_fee  \n",
       "24    -57.600000 -1.000000    0.000000    -61.350000    -1.250000  \n",
       "87     -7.200000  0.000000    0.000000    -11.200000     0.000000  \n",
       "162    -9.300000  0.000000    0.000000    -13.300000     0.000000  \n",
       "252   -70.000000  0.000000    0.000000    -81.800000    -1.250000  \n",
       "474   -55.500000 -1.000000    0.000000    -60.500000     0.000000  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check where values of fare amount are negative\n",
    "def show_negative(df, column):\n",
    "    row_total = df.shape[0]\n",
    "    neg_count = df[df[column] < 0].shape[0]\n",
    "    neg_percent = neg_count / row_total * 100\n",
    "    return print(f'The {column} column contains {neg_count} ({neg_percent:.2f}%) negative values out of {row_total}')\n",
    "\n",
    "\n",
    "negative_df = df.loc[df['fare_amount'] < 0]\n",
    "show_negative(df, 'fare_amount')\n",
    "negative_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNKpDtTh8awi"
   },
   "source": [
    "Did you notice something different in the `RatecodeID` column for above records?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "id": "9eVrrj7c3kjq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RatecodeID\n",
       "1.000000    2335\n",
       "2.000000     151\n",
       "5.000000      53\n",
       "3.000000      36\n",
       "4.000000      18\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse RatecodeID for the negative fare amounts\n",
    "negative_df['RatecodeID'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "id": "Ruyh2vaCRNxr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The passenger_count column contains 0 (0.00%) negative values out of 268150\n",
      "The trip_distance column contains 0 (0.00%) negative values out of 268150\n",
      "The RatecodeID column contains 0 (0.00%) negative values out of 268150\n",
      "The PULocationID column contains 0 (0.00%) negative values out of 268150\n",
      "The DOLocationID column contains 0 (0.00%) negative values out of 268150\n",
      "The payment_type column contains 0 (0.00%) negative values out of 268150\n",
      "The fare_amount column contains 2641 (0.98%) negative values out of 268150\n",
      "The extra column contains 1300 (0.48%) negative values out of 268150\n",
      "The tip_amount column contains 14 (0.01%) negative values out of 268150\n",
      "The total_amount column contains 2610 (0.97%) negative values out of 268150\n",
      "The airport_fee column contains 336 (0.13%) negative values out of 268150\n"
     ]
    }
   ],
   "source": [
    "# Find which columns have negative values\n",
    "numeric_cols = df.select_dtypes(include=['number'])\n",
    "for col in numeric_cols:\n",
    "    show_negative(df, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPehZ6PJg3_J"
   },
   "outputs": [],
   "source": [
    "# fix these negative values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2hakCCy6wXI"
   },
   "source": [
    "### **2.2** Handling Missing Values\n",
    "<font color = red>[10 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-K-QNPDVVhzR"
   },
   "source": [
    "**2.2.1**  <font color = red>[2 marks]</font> <br>\n",
    "Find the proportion of missing values in each column\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfTa9cct6Gec"
   },
   "outputs": [],
   "source": [
    "# Find the proportion of missing values in each column\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UdUl6AL_-E_"
   },
   "source": [
    "**2.2.2**  <font color = red>[3 marks]</font> <br>\n",
    "Handling missing values in `passenger_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmNmhr4q8Xys"
   },
   "outputs": [],
   "source": [
    "# Display the rows with null values\n",
    "# Impute NaN values in 'passenger_count'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIPCyR6UCw0c"
   },
   "source": [
    "Did you find zeroes in passenger_count? Handle these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUr4fwkjBUTQ"
   },
   "source": [
    "**2.2.3**  <font color = red>[2 marks]</font> <br>\n",
    "Handle missing values in `RatecodeID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEgFxytzBkrB"
   },
   "outputs": [],
   "source": [
    "# Fix missing values in 'RatecodeID'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TjHXzuODCUW"
   },
   "source": [
    "**2.2.4**  <font color = red>[3 marks]</font> <br>\n",
    "Impute NaN in `congestion_surcharge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqnabUGC3xOA"
   },
   "outputs": [],
   "source": [
    "# handle null values in congestion_surcharge\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FZEdgAgUbPl"
   },
   "source": [
    "Are there missing values in other columns? Did you find NaN values in some other set of columns? Handle those missing values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDLpyqCRXa3K"
   },
   "outputs": [],
   "source": [
    "# Handle any remaining missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jyQyYIpCztl"
   },
   "source": [
    "### **2.3** Handling Outliers\n",
    "<font color = red>[10 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoVwZzuEMTHB"
   },
   "source": [
    "Before we start fixing outliers, let's perform outlier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHK5K6uV8XpU"
   },
   "outputs": [],
   "source": [
    "# Describe the data and check if there are any potential outliers present\n",
    "# Check for potential out of place values in various columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XDuLkufyJ2g"
   },
   "source": [
    "**2.3.1**  <font color = red>[10 marks]</font> <br>\n",
    "Based on the above analysis, it seems that some of the outliers are present due to errors in registering the trips. Fix the outliers.\n",
    "\n",
    "Some points you can look for:\n",
    "- Entries where `trip_distance` is nearly 0 and `fare_amount` is more than 300\n",
    "- Entries where `trip_distance` and `fare_amount` are 0 but the pickup and dropoff zones are different (both distance and fare should not be zero for different zones)\n",
    "- Entries where `trip_distance` is more than 250  miles.\n",
    "- Entries where `payment_type` is 0 (there is no payment_type 0 defined in the data dictionary)\n",
    "\n",
    "These are just some suggestions. You can handle outliers in any way you wish, using the insights from above outlier analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-YNHI8tea9c"
   },
   "source": [
    "How will you fix each of these values? Which ones will you drop and which ones will you replace?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ap4IfwXO4yZe"
   },
   "source": [
    "First, let us remove 7+ passenger counts as there are very less instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfsOFY5y9-fA"
   },
   "outputs": [],
   "source": [
    "# remove passenger_count > 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCUNe3tu8bie"
   },
   "outputs": [],
   "source": [
    "# Continue with outlier handling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuzQXfuT8YKV"
   },
   "outputs": [],
   "source": [
    "# Do any columns need standardising?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPpJyFFNffcL"
   },
   "source": [
    "## **3** Exploratory Data Analysis\n",
    "<font color = red>[90 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cl-0PcYTfkqh"
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4N3PvkSTwcN"
   },
   "source": [
    "#### **3.1** General EDA: Finding Patterns and Trends\n",
    "<font color = red>[40 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hwonDfZTJO6"
   },
   "source": [
    "**3.1.1** <font color = red>[3 marks]</font> <br>\n",
    "Categorise the varaibles into Numerical or Categorical.\n",
    "* `VendorID`:\n",
    "* `tpep_pickup_datetime`:\n",
    "* `tpep_dropoff_datetime`:\n",
    "* `passenger_count`:\n",
    "* `trip_distance`:\n",
    "* `RatecodeID`:\n",
    "* `PULocationID`:\n",
    "* `DOLocationID`:\n",
    "* `payment_type`:\n",
    "* `pickup_hour`:\n",
    "* `trip_duration`:\n",
    "\n",
    "\n",
    "The following monetary parameters belong in the same category, is it categorical or numerical?\n",
    "\n",
    "\n",
    "* `fare_amount`\n",
    "* `extra`\n",
    "* `mta_tax`\n",
    "* `tip_amount`\n",
    "* `tolls_amount`\n",
    "* `improvement_surcharge`\n",
    "* `total_amount`\n",
    "* `congestion_surcharge`\n",
    "* `airport_fee`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbGMjArkiXjL"
   },
   "source": [
    "##### Temporal Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCCbmQ49s2qj"
   },
   "source": [
    "**3.1.2** <font color = red>[5 marks]</font> <br>\n",
    "Analyse the distribution of taxi pickups by hours, days of the week, and months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwgWN_MWg0Au"
   },
   "outputs": [],
   "source": [
    "# Find and show the hourly trends in taxi pickups\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R64b8zfkG5OK"
   },
   "outputs": [],
   "source": [
    "# Find and show the daily trends in taxi pickups (days of the week)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7V-jmRQG5hJ"
   },
   "outputs": [],
   "source": [
    "# Show the monthly trends in pickups\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23IqsgNjHNpJ"
   },
   "source": [
    "##### Financial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRaO-3NqHSM6"
   },
   "source": [
    "Take a look at the financial parameters like `fare_amount`, `tip_amount`, `total_amount`, and also `trip_distance`. Do these contain zero/negative values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7JUnPeRHRqF"
   },
   "outputs": [],
   "source": [
    "# Analyse the above parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbOS_-GDJTyv"
   },
   "source": [
    "Do you think it is beneficial to create a copy DataFrame leaving out the zero values from these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDN14J63o9gV"
   },
   "source": [
    "**3.1.3** <font color = red>[2 marks]</font> <br>\n",
    "Filter out the zero values from the above columns.\n",
    "\n",
    "**Note:** The distance might be 0 in cases where pickup and drop is in the same zone. Do you think it is suitable to drop such cases of zero distance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mk7Yp41JqJd"
   },
   "outputs": [],
   "source": [
    "# Create a df with non zero entries for the selected parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJcgwyrtKivH"
   },
   "source": [
    "**3.1.4** <font color = red>[3 marks]</font> <br>\n",
    "Analyse the monthly revenue (`total_amount`) trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_gl8rBD6ZH0"
   },
   "outputs": [],
   "source": [
    "# Group data by month and analyse monthly revenue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vQHosTZLalB"
   },
   "source": [
    "**3.1.5** <font color = red>[3 marks]</font> <br>\n",
    "Show the proportion of each quarter of the year in the revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foV9BpFbVTbu"
   },
   "outputs": [],
   "source": [
    "# Calculate proportion of each quarter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JncXEPjBLyHp"
   },
   "source": [
    "**3.1.6** <font color = red>[3 marks]</font> <br>\n",
    "Visualise the relationship between `trip_distance` and `fare_amount`. Also find the correlation value for these two.\n",
    "\n",
    "**Hint:** You can leave out the trips with trip_distance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-z8Fegh_P5At"
   },
   "outputs": [],
   "source": [
    "# Show how trip fare is affected by distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OGNFTX4QdeS"
   },
   "source": [
    "**3.1.7** <font color = red>[5 marks]</font> <br>\n",
    "Find and visualise the correlation between:\n",
    "1. `fare_amount` and trip duration (pickup time to dropoff time)\n",
    "2. `fare_amount` and `passenger_count`\n",
    "3. `tip_amount` and `trip_distance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtSNqFw19TB3"
   },
   "outputs": [],
   "source": [
    "# Show relationship between fare and trip duration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTrlXiBm9TB3"
   },
   "outputs": [],
   "source": [
    "# Show relationship between fare and number of passengers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lph7rakc9TB3"
   },
   "outputs": [],
   "source": [
    "# Show relationship between tip and trip distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EL5CU47QJ5u"
   },
   "source": [
    "**3.1.8** <font color = red>[3 marks]</font> <br>\n",
    "Analyse the distribution of different payment types (`payment_type`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pknOiYLp9Wnd"
   },
   "outputs": [],
   "source": [
    "# Analyse the distribution of different payment types (payment_type).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxxbUmSZ9Wne"
   },
   "source": [
    "- 1= Credit card\n",
    "- 2= Cash\n",
    "- 3= No charge\n",
    "- 4= Dispute\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVXxcSYHj9sh"
   },
   "source": [
    "##### Geographical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvBXuYW7kEyS"
   },
   "source": [
    "For this, you have to use the *taxi_zones.shp* file from the *taxi_zones* folder.\n",
    "\n",
    "There would be multiple files inside the folder (such as *.shx, .sbx, .sbn* etc). You do not need to import/read any of the files other than the shapefile, *taxi_zones.shp*.\n",
    "\n",
    "Do not change any folder structure - all the files need to be present inside the folder for it to work.\n",
    "\n",
    "The folder structure should look like this:\n",
    "```\n",
    "Taxi Zones\n",
    "|- taxi_zones.shp.xml\n",
    "|- taxi_zones.prj\n",
    "|- taxi_zones.sbn\n",
    "|- taxi_zones.shp\n",
    "|- taxi_zones.dbf\n",
    "|- taxi_zones.shx\n",
    "|- taxi_zones.sbx\n",
    "\n",
    " ```\n",
    "\n",
    " You only need to read the `taxi_zones.shp` file. The *shp* file will utilise the other files by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hR8f8ypXUtxJ"
   },
   "source": [
    "We will use the *GeoPandas* library for geopgraphical analysis\n",
    "```\n",
    "import geopandas as gpd\n",
    "```\n",
    "\n",
    "More about geopandas and shapefiles: [About](https://geopandas.org/en/stable/about.html)\n",
    "\n",
    "\n",
    "Reading the shapefile is very similar to *Pandas*. Use `gpd.read_file()` function to load the data (*taxi_zones.shp*) as a GeoDataFrame. Documentation: [Reading and Writing Files](https://geopandas.org/en/stable/docs/user_guide/io.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJ76QD9IXNz8"
   },
   "outputs": [],
   "source": [
    "# !pip install geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_QpZLl_WU-b"
   },
   "source": [
    "**3.1.9** <font color = red>[2 marks]</font> <br>\n",
    "Load the shapefile and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLyVd3TQkCdG"
   },
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "\n",
    "\n",
    "# Read the shapefile using geopandas\n",
    "zones = # read the .shp file using gpd\n",
    "zones.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YXJMzRoWxeg"
   },
   "source": [
    "Now, if you look at the DataFrame created, you will see columns like: `OBJECTID`,`Shape_Leng`, `Shape_Area`, `zone`, `LocationID`, `borough`, `geometry`.\n",
    "<br><br>\n",
    "\n",
    "Now, the `locationID` here is also what we are using to mark pickup and drop zones in the trip records.\n",
    "\n",
    "The geometric parameters like shape length, shape area and geometry are used to plot the zones on a map.\n",
    "\n",
    "This can be easily done using the `plot()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTqdZChLYE6H"
   },
   "outputs": [],
   "source": [
    "# print(zones.info())\n",
    "# zones.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBkWokLIY_BH"
   },
   "source": [
    "Now, you have to merge the trip records and zones data using the location IDs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzUMLUjqaJLN"
   },
   "source": [
    "**3.1.10** <font color = red>[3 marks]</font> <br>\n",
    "Merge the zones data into trip data using the `locationID` and `PULocationID` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2L8hWMQaYkX"
   },
   "outputs": [],
   "source": [
    "# Merge zones and trip records using locationID and PULocationID\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CYc36Weai5-"
   },
   "source": [
    "**3.1.11** <font color = red>[3 marks]</font> <br>\n",
    "Group data by location IDs to find the total number of trips per location ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpTIaPSSbwZZ"
   },
   "outputs": [],
   "source": [
    "# Group data by location and calculate the number of trips\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "As0dVJpdcK2_"
   },
   "source": [
    "**3.1.12** <font color = red>[2 marks]</font> <br>\n",
    "Now, use the grouped data to add number of trips to the GeoDataFrame.\n",
    "\n",
    "We will use this to plot a map of zones showing total trips per zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9Sheg7vdZ6q"
   },
   "outputs": [],
   "source": [
    "# Merge trip counts back to the zones GeoDataFrame\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsfomL6Od0_R"
   },
   "source": [
    "The next step is creating a color map (choropleth map) showing zones by the number of trips taken.\n",
    "\n",
    "Again, you can use the `zones.plot()` method for this. [Plot Method GPD](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.plot.html#geopandas.GeoDataFrame.plot)\n",
    "\n",
    "But first, you need to define the figure and axis for the plot.\n",
    "\n",
    "`fig, ax = plt.subplots(1, 1, figsize = (12, 10))`\n",
    "\n",
    "This function creates a figure (fig) and a single subplot (ax)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgDcw8gUejVk"
   },
   "source": [
    "After setting up the figure and axis, we can proceed to plot the GeoDataFrame on this axis. This is done in the next step where we use the plot method of the GeoDataFrame.\n",
    "\n",
    "You can define the following parameters in the `zones.plot()` method:\n",
    "```\n",
    "column = '',\n",
    "ax = ax,\n",
    "legend = True,\n",
    "legend_kwds = {'label': \"label\", 'orientation': \"<horizontal/vertical>\"}\n",
    "```\n",
    "\n",
    "To display the plot, use `plt.show()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFQmkJipfU1P"
   },
   "source": [
    "**3.1.13** <font color = red>[3 marks]</font> <br>\n",
    "Plot a color-coded map showing zone-wise trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i506kVTgefM5"
   },
   "outputs": [],
   "source": [
    "# Define figure and axis\n",
    "\n",
    "\n",
    "# Plot the map and display it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwDs7OaBSopP"
   },
   "outputs": [],
   "source": [
    "# can you try displaying the zones DF sorted by the number of trips?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1bE7-QbgSrZ"
   },
   "source": [
    "Here we have completed the temporal, financial and geographical analysis on the trip records.\n",
    "\n",
    "**Compile your findings from general analysis below:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YiVFIX3gcL3"
   },
   "source": [
    "You can consider the following points:\n",
    "\n",
    "* Busiest hours, days and months\n",
    "* Trends in revenue collected\n",
    "* Trends in quarterly revenue\n",
    "* How fare depends on trip distance, trip duration and passenger counts\n",
    "* How tip amount depends on trip distance\n",
    "* Busiest zones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rv0oYLcbhOTU"
   },
   "source": [
    "#### **3.2** Detailed EDA: Insights and Strategies\n",
    "<font color = red>[50 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWJk-CGihnU1"
   },
   "source": [
    "Having performed basic analyses for finding trends and patterns, we will now move on to some detailed analysis focussed on operational efficiency, pricing strategies, and customer experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBY2Y4Jrz9IQ"
   },
   "source": [
    "##### Operational Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXU8Q3sGjGnE"
   },
   "source": [
    "Analyze variations by time of day and location to identify bottlenecks or inefficiencies in routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H97VPqVdq7Lz"
   },
   "source": [
    "**3.2.1** <font color = red>[3 marks]</font> <br>\n",
    "Identify slow routes by calculating the average time taken by cabs to get from one zone to another at different hours of the day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzTRZyw2q9IR"
   },
   "source": [
    "Speed on a route *X* for hour *Y* = (*distance of the route X / average trip duration for hour Y*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ovf-1vIyhk7E"
   },
   "outputs": [],
   "source": [
    "# Find routes which have the slowest speeds at different times of the day\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmRPbH3rm2Ub"
   },
   "source": [
    "How does identifying high-traffic, high-demand routes help us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-w-OCB_nLmb"
   },
   "source": [
    "**3.2.2** <font color = red>[3 marks]</font> <br>\n",
    "Calculate the number of trips at each hour of the day and visualise them. Find the busiest hour and show the number of trips for that hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEbRCT31nLUw"
   },
   "outputs": [],
   "source": [
    "# Visualise the number of trips per hour and find the busiest hour\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FuSAA0zn3F4"
   },
   "source": [
    "Remember, we took a fraction of trips. To find the actual number, you have to scale the number up by the sampling ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bapn075QrKaz"
   },
   "source": [
    "**3.2.3** <font color = red>[2 mark]</font> <br>\n",
    "Find the actual number of trips in the five busiest hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79Hy-YWOoapQ"
   },
   "outputs": [],
   "source": [
    "# Scale up the number of trips\n",
    "\n",
    "# Fill in the value of your sampling fraction and use that to scale up the numbers\n",
    "sample_fraction =\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74fhoLxEoaTP"
   },
   "source": [
    "**3.2.4** <font color = red>[3 marks]</font> <br>\n",
    "Compare hourly traffic pattern on weekdays. Also compare for weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYMmCQHwpFRY"
   },
   "outputs": [],
   "source": [
    "# Compare traffic trends for the week days and weekends\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWeuAulNpSOL"
   },
   "source": [
    "What can you infer from the above patterns? How will finding busy and quiet hours for each day help us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-S90VG94rGUm"
   },
   "source": [
    "**3.2.5** <font color = red>[3 marks]</font> <br>\n",
    "Identify top 10 zones with high hourly pickups. Do the same for hourly dropoffs. Show pickup and dropoff trends in these zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nJGifJFrslP"
   },
   "outputs": [],
   "source": [
    "# Find top 10 pickup and dropoff zones\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okkQ17VssMqP"
   },
   "source": [
    "**3.2.6** <font color = red>[3 marks]</font> <br>\n",
    "Find the ratio of pickups and dropoffs in each zone. Display the 10 highest (pickup/drop) and 10 lowest (pickup/drop) ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qbuc8y-KgeDR"
   },
   "outputs": [],
   "source": [
    "# Find the top 10 and bottom 10 pickup/dropoff ratios\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2j1ukv-rrxny"
   },
   "source": [
    "**3.2.7** <font color = red>[3 marks]</font> <br>\n",
    "Identify zones with high pickup and dropoff traffic during night hours (11PM to 5AM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ve65f0GltyE_"
   },
   "outputs": [],
   "source": [
    "# During night hours (11pm to 5am) find the top 10 pickup and dropoff zones\n",
    "# Note that the top zones should be of night hours and not the overall top zones\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtPTHyhTNnNL"
   },
   "source": [
    "Now, let us find the revenue share for the night time hours and the day time hours. After this, we will move to deciding a pricing strategy.\n",
    "\n",
    "**3.2.8** <font color = red>[2 marks]</font> <br>\n",
    "Find the revenue share for nighttime and daytime hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ohk4curNl3g"
   },
   "outputs": [],
   "source": [
    "# Filter for night hours (11 PM to 5 AM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nG77smP4tyWu"
   },
   "source": [
    "##### Pricing Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciF6eThit5IH"
   },
   "source": [
    "**3.2.9** <font color = red>[2 marks]</font> <br>\n",
    "For the different passenger counts, find the average fare per mile per passenger.\n",
    "\n",
    "For instance, suppose the average fare per mile for trips with 3 passengers is 3 USD/mile, then the fare per mile per passenger will be 1 USD/mile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AO7MIuXd9C16"
   },
   "outputs": [],
   "source": [
    "# Analyse the fare per mile per passenger for different passenger counts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZrN-cxR9DIb"
   },
   "source": [
    "**3.2.10** <font color = red>[3 marks]</font> <br>\n",
    "Find the average fare per mile by hours of the day and by days of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUzCJm2y9SOq"
   },
   "outputs": [],
   "source": [
    "# Compare the average fare per mile for different days and for different times of the day\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "211WFkhHDUMG"
   },
   "source": [
    "**3.2.11** <font color = red>[3 marks]</font> <br>\n",
    "Analyse the average fare per mile for the different vendors for different hours of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "my3cGHL1DU04"
   },
   "outputs": [],
   "source": [
    "# Compare fare per mile for different vendors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH9YJdodtDk4"
   },
   "source": [
    "**3.2.12** <font color = red>[5 marks]</font> <br>\n",
    "Compare the fare rates of the different vendors in a tiered fashion. Analyse the average fare per mile for distances upto 2 miles. Analyse the fare per mile for distances from 2 to 5 miles. And then for distances more than 5 miles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFRPgO7mgEeT"
   },
   "outputs": [],
   "source": [
    "# Defining distance tiers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2o6xY0Yh6Gv"
   },
   "source": [
    "##### Customer Experience and Other Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSneN-rCh6Gw"
   },
   "source": [
    "**3.2.13** <font color = red>[5 marks]</font> <br>\n",
    "Analyse average tip percentages based on trip distances, passenger counts and time of pickup. What factors lead to low tip percentages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LO5WazK8h6Gx"
   },
   "outputs": [],
   "source": [
    "#  Analyze tip percentages based on distances, passenger counts and pickup times\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQAIQsrcLm6X"
   },
   "source": [
    "Additional analysis [optional]: Let's try comparing cases of low tips with cases of high tips to find out if we find a clear aspect that drives up the tipping behaviours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "QYoUed6Dn6YW"
   },
   "outputs": [],
   "source": [
    "# Compare trips with tip percentage < 10% to trips with tip percentage > 25%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UjvGTRKh6Gx"
   },
   "source": [
    "**3.2.14** <font color = red>[3 marks]</font> <br>\n",
    "Analyse the variation of passenger count across hours and days of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFP0DNRvh6Gx"
   },
   "outputs": [],
   "source": [
    "# See how passenger count varies across hours and days\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWKLLlb7h6Gy"
   },
   "source": [
    "**3.2.15** <font color = red>[2 marks]</font> <br>\n",
    "Analyse the variation of passenger counts across zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30h27Uz2h6Gy"
   },
   "outputs": [],
   "source": [
    "# How does passenger count vary across zones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3I0AfOkvBWA"
   },
   "outputs": [],
   "source": [
    "# For a more detailed analysis, we can use the zones_with_trips GeoDataFrame\n",
    "# Create a new column for the average passenger count in each zone.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rw0j5Z-1h6Gz"
   },
   "source": [
    "Find out how often surcharges/extra charges are applied to understand their prevalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9IHjHFBx6Ye"
   },
   "source": [
    "**3.2.16** <font color = red>[5 marks]</font> <br>\n",
    "Analyse the pickup/dropoff zones or times when extra charges are applied more frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKbXwZJwh6Gz"
   },
   "outputs": [],
   "source": [
    "# How often is each surcharge applied?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkyc9q40Cts-"
   },
   "source": [
    "## **4** Conclusion\n",
    "<font color = red>[15 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5yCODI-C6yR"
   },
   "source": [
    "### **4.1** Final Insights and Recommendations\n",
    "<font color = red>[15 marks]</font> <br>\n",
    "\n",
    "Conclude your analyses here. Include all the outcomes you found based on the analysis.\n",
    "\n",
    "Based on the insights, frame a concluding story explaining suitable parameters such as location, time of the day, day of the week etc. to be kept in mind while devising a strategy to meet customer demand and optimise supply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dy8J-C8jJjk"
   },
   "source": [
    "**4.1.1** <font color = red>[5 marks]</font> <br>\n",
    "Recommendations to optimize routing and dispatching based on demand patterns and operational inefficiencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J03px17x_rD9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaLgTWxpjt7h"
   },
   "source": [
    "**4.1.2** <font color = red>[5 marks]</font> <br>\n",
    "\n",
    "Suggestions on strategically positioning cabs across different zones to make best use of insights uncovered by analysing trip trends across time, days and months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8ZbTIF7_rsN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUU4mn50jMZy"
   },
   "source": [
    "**4.1.3** <font color = red>[5 marks]</font> <br>\n",
    "Propose data-driven adjustments to the pricing strategy to maximize revenue while maintaining competitive rates with other vendors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXErHFjx_sGN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "-OVfUMlHFkZD",
    "0eaCZjHIvfuI",
    "Kw-WRzBfyS7j",
    "nM2X-s6lycvQ",
    "NgHgbPIepaYl",
    "QaOS3H9izZ0N",
    "HZvPSwJx0S3K",
    "x2hakCCy6wXI",
    "2jyQyYIpCztl",
    "gPpJyFFNffcL",
    "a4N3PvkSTwcN",
    "MbGMjArkiXjL",
    "23IqsgNjHNpJ",
    "CVXxcSYHj9sh",
    "Rv0oYLcbhOTU",
    "XBY2Y4Jrz9IQ",
    "nG77smP4tyWu",
    "M2o6xY0Yh6Gv",
    "bkyc9q40Cts-",
    "Z5yCODI-C6yR"
   ],
   "provenance": [
    {
     "file_id": "1PvYLECban8pmD-aFGV_yoRF2PyRwLntw",
     "timestamp": 1737110016234
    },
    {
     "file_id": "1jppXTxMvUcVsY27R_ckdE5DAnxrROUY1",
     "timestamp": 1732620370544
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
